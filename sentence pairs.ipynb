{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39259,
     "status": "ok",
     "timestamp": 1559005570663,
     "user": {
      "displayName": "朱莹浩",
      "photoUrl": "",
      "userId": "09830692503690873709"
     },
     "user_tz": -600
    },
    "id": "u_H0bEro0-X1",
    "outputId": "a1bb0c51-7950-4a40-a47d-0c90406743d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 171133,
     "status": "ok",
     "timestamp": 1559005743996,
     "user": {
      "displayName": "朱莹浩",
      "photoUrl": "",
      "userId": "09830692503690873709"
     },
     "user_tz": -600
    },
    "id": "XoMYrph6Hry6",
    "outputId": "177443db-1f42-417e-e6b5-ec39ffd9616e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[==============================================----] 92.6% 348.4/376.1MB downloaded"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from time import time\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import itertools\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Lambda,Dropout\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "import keras\n",
    "\n",
    "word_embed_model = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5595,
     "status": "ok",
     "timestamp": 1559006008448,
     "user": {
      "displayName": "朱莹浩",
      "photoUrl": "",
      "userId": "09830692503690873709"
     },
     "user_tz": -600
    },
    "id": "BbXYsyLl1EWX",
    "outputId": "54045159-0d5c-4e62-fd7c-48f98efd8a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 3)\n",
      "(120581, 6)\n",
      "(584843, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/train4.csv')\n",
    "dev_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/devset.csv')\n",
    "test_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/testset3.csv')\n",
    "\n",
    "print(train_df.shape)\n",
    "print(dev_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lRW0jVP1GL7"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "def change_label(df):\n",
    "  labels = df['label']\n",
    "  new_labels = []\n",
    "  for row in labels:\n",
    "    if row == 'SUPPORTS':\n",
    "      new_labels.append(2)\n",
    "    elif row == 'REFUTES':\n",
    "      new_labels.append(1)\n",
    "    else:\n",
    "      new_labels.append(0)\n",
    "  df['label'] = pd.Series(new_labels)\n",
    "\n",
    "def process_text(raw_text):\n",
    "    text = str(raw_text)\n",
    "    text = re.sub('_', ' ', text)\n",
    "    text = re.sub('-LRB-', ' ', text)\n",
    "    text = re.sub('-RRB-', ' ', text)\n",
    "    text = re.sub('-LSB-', ' ', text)\n",
    "    text = re.sub('-RSB-', ' ', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "change_label(train_df)\n",
    "change_label(dev_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2576715,
     "status": "ok",
     "timestamp": 1559008620534,
     "user": {
      "displayName": "朱莹浩",
      "photoUrl": "",
      "userId": "09830692503690873709"
     },
     "user_tz": -600
    },
    "id": "qCO3YyXS1R-M",
    "outputId": "a1cb7eeb-f15e-4609-e84d-0c0376a190d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "vocab_index = ['<unk>']\n",
    "\n",
    "#substitute the sentence in the train data into the index representation\n",
    "for index, row in train_df.iterrows():\n",
    "  if index%100000 == 0:\n",
    "    print(index)\n",
    "  for sentence in ['claim', 'evidence']:\n",
    "    number_rep = []\n",
    "    for word in process_text(row[sentence]):\n",
    "      if word in stopwords.words('english') and word not in word_embed_model.vocab:\n",
    "        continue\n",
    "      if word not in vocab:\n",
    "        vocab[word] = len(vocab_index)\n",
    "        number_rep.append(len(vocab_index))\n",
    "        vocab_index.append(word)\n",
    "      else:\n",
    "        number_rep.append(vocab[word])\n",
    "    train_df.at[index, sentence] = number_rep\n",
    "    \n",
    "#substitute the sentence in the test data into the index representation\n",
    "for index, row in test_df.iterrows():\n",
    "  if index%100000 == 0:\n",
    "    print(index)\n",
    "  for sentence in ['claim', 'evidence']:\n",
    "    number_rep = []\n",
    "    for word in process_text(row[sentence]):\n",
    "      if word in stopwords.words('english') and word not in word_embed_model.vocab:\n",
    "        continue\n",
    "      if word not in vocab:\n",
    "        vocab[word] = len(vocab_index)\n",
    "        number_rep.append(len(vocab_index))\n",
    "        vocab_index.append(word)\n",
    "      else:\n",
    "        number_rep.append(vocab[word])\n",
    "    test_df.at[index, sentence] = number_rep\n",
    "    \n",
    "    \n",
    "#embedding all vocabulary in a matrix\n",
    "dimension = 300        \n",
    "embeddings = 1 * np.random.randn(len(vocab) + 1,dimension)\n",
    "embeddings[0] = 0 \n",
    "for word, index in vocab.items():\n",
    "  if word in word_embed_model.vocab:\n",
    "    embeddings[index] = word_embed_model[word]\n",
    "\n",
    "change_label(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 427339,
     "status": "ok",
     "timestamp": 1559011342666,
     "user": {
      "displayName": "朱莹浩",
      "photoUrl": "",
      "userId": "09830692503690873709"
     },
     "user_tz": -600
    },
    "id": "7Vkv2pGxDAYW",
    "outputId": "e2cd8e57-42d0-4b13-dd15-7ab96b097bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "#substitute the sentence in the train data into the index representation\n",
    "for index, row in dev_df.iterrows():\n",
    "  if index%100000 == 0:\n",
    "    print(index)\n",
    "  for sentence in ['claim', 'evidence']:\n",
    "    number_rep = []\n",
    "    for word in process_text(row[sentence]):\n",
    "      if word in stopwords.words('english') and word not in word_embed_model.vocab:\n",
    "        continue\n",
    "      if word not in vocab:\n",
    "        vocab[word] = len(vocab_index)\n",
    "        number_rep.append(len(vocab_index))\n",
    "        vocab_index.append(word)\n",
    "      else:\n",
    "        number_rep.append(vocab[word])\n",
    "    dev_df.at[index, sentence] = number_rep\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 384387,
     "status": "ok",
     "timestamp": 1559013196629,
     "user": {
      "displayName": "朱莹浩",
      "photoUrl": "",
      "userId": "09830692503690873709"
     },
     "user_tz": -600
    },
    "id": "G66M7CUfhdbo",
    "outputId": "a6931fbd-378a-4489-bf6c-698f1eb4d207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "test2_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/new_test.csv')\n",
    "\n",
    "#substitute the sentence in the train data into the index representation\n",
    "for index, row in test2_df.iterrows():\n",
    "  if index%100000 == 0:\n",
    "    print(index)\n",
    "  for sentence in ['claim', 'evidence']:\n",
    "    number_rep = []\n",
    "    for word in process_text(row[sentence]):\n",
    "      if word in stopwords.words('english') and word not in word_embed_model.vocab:\n",
    "        continue\n",
    "      if word not in vocab:\n",
    "        vocab[word] = len(vocab_index)\n",
    "        number_rep.append(len(vocab_index))\n",
    "        vocab_index.append(word)\n",
    "      else:\n",
    "        number_rep.append(vocab[word])\n",
    "    test2_df.at[index, sentence] = number_rep\n",
    "test2_df.to_csv(\"new_test_emb.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3keW93Lh2WL0"
   },
   "outputs": [],
   "source": [
    "def max_sequence(df1,df2):\n",
    "    max_seq = max(df1.claim.map(lambda x: len(x)).max(), df1.evidence.map(lambda x: len(x)).max(),df2.claim.map(lambda x: len(x)).max(), df2.evidence.map(lambda x: len(x)).max())\n",
    "    return max_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kE3ysgna5Zpn"
   },
   "outputs": [],
   "source": [
    "# Build a neural network\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Concatenate\n",
    "def manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "n_hidden = 100\n",
    "n_epoch = 100\n",
    "batch_size = 2048\n",
    "\n",
    "max_seq = max_sequence(train_df, test_df)\n",
    "left_input = Input(shape=(max_seq,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq,), dtype='int32')\n",
    "embedding_layer = Embedding(len(embeddings), dimension, weights=[embeddings], input_length=max_seq, trainable=False)\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "shared_lstm = LSTM(n_hidden)\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "malstm_distance = Lambda(function=lambda x: manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "con = Concatenate()([left_output, right_output, malstm_distance])\n",
    "dense1 = Dense(128, activation='relu', input_dim=201)(con)\n",
    "drop1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(3, activation='softmax')(drop1)\n",
    "model =Model([left_input, right_input], dense2)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLNpSjc1BMNN"
   },
   "outputs": [],
   "source": [
    "# Preparing training data\n",
    "cols = ['claim','evidence']\n",
    "x = train_df[cols]\n",
    "y = train_df['label']\n",
    "x_val = dev_df[cols]\n",
    "y_val = dev_df['label']\n",
    "# v_size = 15000\n",
    "# t_size = len(train_df) - v_size\n",
    "    \n",
    "# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=v_size)\n",
    "x = {'claim': x.claim, 'evidence': x.evidence}\n",
    "x_val = {'claim': x_val.claim, 'evidence': x_val.evidence}\n",
    "\n",
    "y = y.values\n",
    "y_val = y_val.values\n",
    "\n",
    "for dataset, side in itertools.product([x, x_val], ['claim', 'evidence']):\n",
    "  dataset[side] = pad_sequences(dataset[side], maxlen=max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VtqGbV-PV-TC"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',np.unique(y), y)\n",
    "\n",
    "y = keras.utils.to_categorical(y, num_classes=3, dtype ='int32')\n",
    "\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wq4fD1G8I0au"
   },
   "outputs": [],
   "source": [
    "# Draw pychart\n",
    "from keras.callbacks import Callback \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib \n",
    "\n",
    "matplotlib.use('Agg') \n",
    "class LossHistory(Callback): \n",
    "\n",
    "   def on_train_begin(self, logs={}): \n",
    "       self.losses = {'batch':[], 'epoch':[]} \n",
    "       self.accuracy = {'batch':[], 'epoch':[]} \n",
    "       self.val_loss = {'batch':[], 'epoch':[]} \n",
    "       self.val_acc = {'batch':[], 'epoch':[]} \n",
    "\n",
    "   def on_batch_end(self, batch, logs={}): \n",
    "       self.losses['batch'].append(logs.get('loss')) \n",
    "       self.accuracy['batch'].append(logs.get('acc')) \n",
    "       self.val_loss['batch'].append(logs.get('val_loss')) \n",
    "       self.val_acc['batch'].append(logs.get('val_acc')) \n",
    "\n",
    "   def on_epoch_end(self, batch, logs={}): \n",
    "       self.losses['epoch'].append(logs.get('loss')) \n",
    "       self.accuracy['epoch'].append(logs.get('acc')) \n",
    "       self.val_loss['epoch'].append(logs.get('val_loss')) \n",
    "       self.val_acc['epoch'].append(logs.get('val_acc')) \n",
    "\n",
    "   def loss_plot(self, savepath): \n",
    "       iters = range(len(self.losses[\"epoch\"])) \n",
    "       plt.figure() \n",
    "       # acc \n",
    "       plt.plot(iters, self.accuracy[\"epoch\"], 'r', label='train acc') \n",
    "       # loss \n",
    "       plt.plot(iters, self.losses[\"epoch\"], 'g', label='train loss') \n",
    "       \n",
    "       plt.grid(True) \n",
    "       plt.xlabel('epoch') \n",
    "       plt.ylabel('acc-loss') \n",
    "       plt.legend(loc=\"upper right\")  \n",
    "       plt.savefig(savepath) \n",
    "    \n",
    "   def val_loss_plot(self, savepath): \n",
    "   \n",
    "       iters = range(len(self.losses[\"epoch\"])) \n",
    "       plt.figure() \n",
    "      \n",
    "       # val_acc \n",
    "       plt.plot(iters, self.val_acc[\"epoch\"], 'b', label='val acc') \n",
    "       # val_loss \n",
    "       plt.plot(iters, self.val_loss[\"epoch\"], 'k', label='val loss') \n",
    "       plt.grid(True) \n",
    "       plt.xlabel('epoach') \n",
    "       plt.ylabel('acc-loss') \n",
    "       plt.legend(loc=\"upper right\")  \n",
    "       plt.savefig(savepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 765,
     "status": "error",
     "timestamp": 1559015676336,
     "user": {
      "displayName": "朱莹浩",
      "photoUrl": "",
      "userId": "09830692503690873709"
     },
     "user_tz": -600
    },
    "id": "kGKbnMe2pYgU",
    "outputId": "a632d82f-42ee-4164-abec-432c562a844b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f02ce0dfef9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLossHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/yinghao/yinghao.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"claim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"evidence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/yinghao/train.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModelCheckpoint' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "start = time()\n",
    "history = LossHistory()\n",
    "model_checkpoint = ModelCheckpoint('/content/gdrive/My Drive/yinghao/yinghao.hdf5', monitor='loss', verbose=1, save_best_only=True)\n",
    "train = model.fit([x[\"claim\"], x[\"evidence\"]], y, epochs=n_epoch, batch_size=batch_size)\n",
    "history.loss_plot(\"/content/gdrive/My Drive/yinghao/train.png\")\n",
    "history.val_loss_plot(\"/content/gdrive/My Drive/yinghao/val.png\")\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16145,
     "status": "ok",
     "timestamp": 1558962769094,
     "user": {
      "displayName": "朱莹浩",
      "photoUrl": "",
      "userId": "09830692503690873709"
     },
     "user_tz": -600
    },
    "id": "47yXWxKMg_xr",
    "outputId": "90ce5ab4-ad03-4f24-c525-bbd35ec07767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107244, 3)\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "x_test = test2_df[cols]\n",
    "x_test = {'claim': x_test.claim, 'evidence': x_test.evidence}\n",
    "for dataset, side in itertools.product([x_test], ['claim', 'evidence']):\n",
    "  dataset[side] = pad_sequences(dataset[side], maxlen=max_seq)\n",
    "  \n",
    "predict = model.predict([x_test[\"claim\"], x_test[\"evidence\"]],batch_size=2048)\n",
    "print(predict.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AYwXwMUPPyNN"
   },
   "outputs": [],
   "source": [
    "def find_max_pos(array):\n",
    "  m = max(array)\n",
    "  p = 0\n",
    "  for i in range(0,len(array)):\n",
    "    if (array[i] == m):\n",
    "      p = i\n",
    "  return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 958,
     "status": "ok",
     "timestamp": 1558962788754,
     "user": {
      "displayName": "朱莹浩",
      "photoUrl": "",
      "userId": "09830692503690873709"
     },
     "user_tz": -600
    },
    "id": "oIEbvOdWSH-k",
    "outputId": "4d80ed4e-e07c-4dbb-873e-6fbc46329ac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107244\n"
     ]
    }
   ],
   "source": [
    "test_label = []\n",
    "for array in predict:\n",
    "  pos = find_max_pos(array)\n",
    "  test_label.append(pos)\n",
    "\n",
    "print(len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1035,
     "status": "ok",
     "timestamp": 1558962794510,
     "user": {
      "displayName": "朱莹浩",
      "photoUrl": "",
      "userId": "09830692503690873709"
     },
     "user_tz": -600
    },
    "id": "VjQyPkFCSmib",
    "outputId": "de8e0697-f736-4dff-d682-b6715d9957ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 60787, 1: 23823, 2: 22634})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zf47YSpgUi8c"
   },
   "outputs": [],
   "source": [
    "# generate json file for test set\n",
    "r = {}\n",
    "cur_key = \"\"\n",
    "for index, row in result_df.iterrows():\n",
    "  # just update evidence when the key does not change\n",
    "  if row['key'] == cur_key:\n",
    "    tmp = []\n",
    "    tmp.append(row['title'])\n",
    "    tmp.append(int(row['page']))\n",
    "    tmp.append(row['label'])\n",
    "    r[cur_key]['evidence'].append(tmp)\n",
    "  \n",
    "  # when key changed, we need add new member\n",
    "  else:\n",
    "    cur_key = row['key']\n",
    "    r[cur_key] = {}\n",
    "    if row['evidence'] == 'None':\n",
    "      r[cur_key]['claim'] = r[cur_key].setdefault('claim', row['claim'])\n",
    "      r[cur_key]['evidence'] =  r[cur_key].setdefault('evidence', [])\n",
    "        \n",
    "    else:\n",
    "      r[cur_key]['claim'] = r[cur_key].setdefault('claim', row['claim'])\n",
    "      r[cur_key]['evidence'] =  r[cur_key].setdefault('evidence', [])\n",
    "      tmp = []\n",
    "      tmp.append(row['title'])\n",
    "      tmp.append(int(row['page']))\n",
    "      tmp.append(row['label'])\n",
    "      r[cur_key]['evidence'].append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "saMk6K3eUnIT"
   },
   "outputs": [],
   "source": [
    "keys = r.keys()\n",
    "for key in keys:\n",
    "  # remove not enough info\n",
    "  if len(r[key]['evidence']) > 0:\n",
    "    for item in r[key]['evidence'].copy():\n",
    "      # if tagged not enough then remove\n",
    "      if item[2] == 0:\n",
    "        r[key]['evidence'].remove(item)\n",
    "    \n",
    "    if len(r[key]['evidence']) == 0:\n",
    "      r[key]['label'] = r[key].setdefault('label', \"NOT ENOUGH INFO\")\n",
    "      \n",
    "    else:\n",
    "      temp = set()\n",
    "      for item in r[key]['evidence']:\n",
    "        temp.add(item[2])\n",
    "      if len(temp) == 2:\n",
    "        r[key]['label'] = r[key].setdefault('label', \"NOT ENOUGH INFO\")\n",
    "        r[key]['evidence'] = []\n",
    "      elif len(temp) == 1:\n",
    "        l = list(temp)[0]\n",
    "        if l == 1:\n",
    "          r[key]['label'] = \"REFUTES\"\n",
    "          for i in range(len(r[key]['evidence'])):\n",
    "            r[key]['evidence'][i] = r[key]['evidence'][i][:2]\n",
    "        elif l == 2:\n",
    "          r[key]['label'] =  \"SUPPORTS\"\n",
    "          for i in range(len(r[key]['evidence'])):\n",
    "            r[key]['evidence'][i] = r[key]['evidence'][i][:2]\n",
    "  else:\n",
    "    r[key]['label'] = r[key].setdefault('label', \"NOT ENOUGH INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ppSXp2AMVBmO"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json_str = json.dumps(r, indent=4)\n",
    "with open('newoutput.json', 'w') as json_file:\n",
    "    json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 847,
     "status": "ok",
     "timestamp": 1558962950661,
     "user": {
      "displayName": "朱莹浩",
      "photoUrl": "",
      "userId": "09830692503690873709"
     },
     "user_tz": -600
    },
    "id": "ZuVoETV9V3AS",
    "outputId": "5fbe3299-266c-4aff-91f9-d282bdea93b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14997\n"
     ]
    }
   ],
   "source": [
    "tmp = json.load(open('newoutput.json','r'))\n",
    "print(len(list(tmp.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vaQwI52MdUqk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sentence pairs.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
